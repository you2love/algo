<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BPç®—æ³• - ç®—æ³•å­¦ä¹ æŒ‡å—</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../lib/prism/prism-tomorrow.min.css">
    <link rel="stylesheet" href="../lib/prism/prism-line-numbers.min.css">
    <style>
        .neural-diagram {
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
            margin: 30px 0;
            padding: 30px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 15px;
        }
        
        .layer-row {
            display: flex;
            align-items: center;
            gap: 40px;
        }
        
        .neuron {
            width: 60px;
            height: 60px;
            background: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            color: #2c3e50;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
            position: relative;
        }
        
        .neuron.input { background: #3498db; color: white; }
        .neuron.hidden { background: #9b59b6; color: white; }
        .neuron.output { background: #e74c3c; color: white; }
        
        .layer-label {
            color: white;
            font-size: 14px;
            margin-bottom: 5px;
        }
        
        .connection {
            position: absolute;
            height: 2px;
            background: rgba(255,255,255,0.5);
            transform-origin: left center;
        }
        
        .forward-arrow {
            font-size: 30px;
            color: white;
            margin: 0 10px;
        }
        
        .gradient-flow {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            margin: 30px 0;
            padding: 30px;
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
            border-radius: 15px;
        }
        
        .gradient-step {
            background: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
        }
        
        .gradient-step .step-num {
            background: #11998e;
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 0 auto 10px;
            font-weight: bold;
        }
        
        .math-formula {
            background: #f8f9fa;
            border-left: 4px solid #3498db;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Times New Roman', serif;
            font-size: 1.1em;
            border-radius: 0 8px 8px 0;
        }
        
        .concept-box {
            background: #f8f9fa;
            border-left: 4px solid #3498db;
            padding: 20px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        .comparison-table th,
        .comparison-table td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        
        .comparison-table th {
            background: #3498db;
            color: white;
        }
        
        .comparison-table tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        .highlight-text {
            background: #fff3cd;
            padding: 2px 6px;
            border-radius: 3px;
        }
        
        .activation-visual {
            display: flex;
            justify-content: center;
            gap: 30px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        
        .activation-func {
            background: #0d1117;
            color: #c9d1d9;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            border: 1px solid #30363d;
            min-width: 200px;
        }
        
        .activation-func h4 {
            color: #58a6ff;
            margin-bottom: 10px;
        }
        
        .activation-func .formula {
            font-family: 'Times New Roman', serif;
            font-size: 1.2em;
            margin: 10px 0;
        }
        
        .activation-func .graph {
            height: 60px;
            display: flex;
            align-items: flex-end;
            justify-content: center;
            gap: 3px;
            margin-top: 10px;
        }
        
        .bar {
            width: 8px;
            background: linear-gradient(180deg, #58a6ff, #1f6feb);
            border-radius: 2px;
        }
        
        .loss-visual {
            display: flex;
            justify-content: center;
            align-items: flex-end;
            gap: 10px;
            height: 100px;
            margin: 20px 0;
            padding: 20px;
            background: #f0f4f8;
            border-radius: 10px;
        }
        
        .loss-bar {
            width: 30px;
            background: linear-gradient(180deg, #e74c3c, #c0392b);
            border-radius: 5px 5px 0 0;
            transition: height 0.3s ease;
        }
        
        .weight-matrix {
            display: flex;
            justify-content: center;
            margin: 20px 0;
        }
        
        .matrix {
            display: inline-block;
            background: #0d1117;
            color: #c9d1d9;
            padding: 15px;
            border-radius: 8px;
            font-family: monospace;
        }
        
        .matrix-row {
            display: flex;
            justify-content: center;
            gap: 10px;
        }
        
        .matrix-cell {
            width: 50px;
            text-align: center;
            padding: 5px;
            border: 1px solid #30363d;
        }
    </style>
</head>
<body>
    <nav class="navbar">
        <div class="container">
            <h1 class="logo">AlgoLearn</h1>
            <ul class="nav-links">
                <li><a href="../index.html">é¦–é¡µ</a></li>
                <li><a href="../index.html#algorithms">ç®—æ³•åˆ†ç±»</a></li>
                <li><a href="../index.html#practice">åœ¨çº¿ç»ƒä¹ </a></li>
            </ul>
        </div>
    </nav>

    <section class="algorithm-detail">
        <div class="container">
            <a href="../index.html#algorithms" class="back-link">â† è¿”å›ç®—æ³•åˆ†ç±»</a>
            <h2>BPç®—æ³• - åå‘ä¼ æ’­</h2>
            
            <div class="algorithm-content">
                <h3>ä»€ä¹ˆæ˜¯BPç®—æ³•ï¼Ÿ</h3>
                <p>
                    <strong>BP (BackPropagation) ç®—æ³•</strong> æ˜¯ç¥ç»ç½‘ç»œä¸­ç”¨äºè®­ç»ƒå¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ï¼Œ
                    é€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºç½‘ç»œå‚æ•°çš„æ¢¯åº¦æ¥æ›´æ–°æƒé‡ã€‚
                </p>
                <p>
                    BP ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š<span class="highlight-text">å°†è¾“å‡ºå±‚çš„è¯¯å·®åå‘ä¼ æ’­åˆ°éšè—å±‚ï¼Œ
                    é€å±‚è®¡ç®—è¯¯å·®æ¢¯åº¦ï¼Œä»è€Œæ›´æ–°ç½‘ç»œæƒé‡</span>ã€‚
                </p>
            </div>

            <div class="complexity">
                <div class="complexity-item">
                    <h4>æ—¶é—´å¤æ‚åº¦</h4>
                    <p>O(E Ã— n Ã— m)</p>
                </div>
                <div class="complexity-item">
                    <h4>ç©ºé—´å¤æ‚åº¦</h4>
                    <p>O(n Ã— m)</p>
                </div>
                <div class="complexity-item">
                    <h4>æ”¶æ•›æ€§</h4>
                    <p>éå‡¸ä¼˜åŒ–</p>
                </div>
            </div>

            <h2>æ ¸å¿ƒåŸç†ï¼šé“¾å¼æ³•åˆ™</h2>
            
            <div class="concept-box">
                <h4>ğŸ¯ ä¸ºä»€ä¹ˆå«"åå‘ä¼ æ’­"ï¼Ÿ</h4>
                <p>
                    <strong>å‰å‘ä¼ æ’­</strong>ï¼šè¾“å…¥ â†’ éšè—å±‚ â†’ è¾“å‡ºå±‚ï¼ˆè®¡ç®—é¢„æµ‹å€¼ï¼‰<br>
                    <strong>åå‘ä¼ æ’­</strong>ï¼šè¾“å‡ºå±‚ â†’ éšè—å±‚ â†’ è¾“å…¥å±‚ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰
                </p>
                <p>
                    BP ç®—æ³•åˆ©ç”¨ <span class="highlight-text">é“¾å¼æ³•åˆ™</span> é€å±‚è®¡ç®—åå¯¼æ•°ï¼Œ
                    å°†æŸå¤±å‡½æ•°çš„è¯¯å·®"åˆ†é…"åˆ°æ¯ä¸ªæƒé‡ä¸Šã€‚
                </p>
            </div>

            <h2>ç½‘ç»œç»“æ„</h2>
            <div class="neural-diagram">
                <div class="layer-row">
                    <div>
                        <div class="layer-label">è¾“å…¥å±‚ (Input)</div>
                        <div class="neuron input">xâ‚</div>
                    </div>
                    <div class="forward-arrow">â†’</div>
                    <div>
                        <div class="layer-label">éšè—å±‚ (Hidden)</div>
                        <div class="neuron hidden">hâ‚</div>
                    </div>
                    <div class="forward-arrow">â†’</div>
                    <div>
                        <div class="layer-label">è¾“å‡ºå±‚ (Output)</div>
                        <div class="neuron output">Å·</div>
                    </div>
                </div>
                <p style="color: white; margin-top: 10px;">
                    3å±‚å‰é¦ˆç¥ç»ç½‘ç»œç»“æ„ç¤ºæ„
                </p>
            </div>

            <h3>ç½‘ç»œå‚æ•°</h3>
            <div class="algorithm-content">
                <ul>
                    <li><strong>è¾“å…¥å±‚</strong>ï¼šæ¥æ”¶åŸå§‹ç‰¹å¾ x âˆˆ â„â¿</li>
                    <li><strong>éšè—å±‚</strong>ï¼šh = f(Wâ‚x + bâ‚)ï¼Œå…¶ä¸­ Wâ‚ æ˜¯è¾“å…¥åˆ°éšè—çš„æƒé‡</li>
                    <li><strong>è¾“å‡ºå±‚</strong>ï¼šÅ· = g(Wâ‚‚h + bâ‚‚)ï¼Œå…¶ä¸­ Wâ‚‚ æ˜¯éšè—åˆ°è¾“å‡ºçš„æƒé‡</li>
                    <li><strong>æ¿€æ´»å‡½æ•°</strong>ï¼šf å’Œ g é€šå¸¸ä½¿ç”¨ ReLUã€Sigmoid æˆ– Tanh</li>
                </ul>
            </div>

            <h2>æ•°å­¦åŸç†è¯¦è§£</h2>

            <h3>1. å‰å‘ä¼ æ’­</h3>
            <div class="algorithm-content">
                <p>å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œè®¡ç®—ç½‘ç»œçš„è¾“å‡ºï¼š</p>
                <div class="math-formula">
                    <p><strong>éšè—å±‚</strong>ï¼šzâ‚ = Wâ‚x + bâ‚ï¼Œh = Ïƒ(zâ‚)</p>
                    <p><strong>è¾“å‡ºå±‚</strong>ï¼šzâ‚‚ = Wâ‚‚h + bâ‚‚ï¼ŒÅ· = softmax(zâ‚‚)</p>
                    <br>
                    <p>å…¶ä¸­ Ïƒ æ˜¯æ¿€æ´»å‡½æ•°ï¼Œsoftmax ç”¨äºå¤šåˆ†ç±»ï¼š</p>
                    <p>softmax(záµ¢) = exp(záµ¢) / Î£â±¼ exp(zâ±¼)</p>
                </div>
            </div>

            <h3>2. æŸå¤±å‡½æ•°</h3>
            <div class="algorithm-content">
                <p>å¯¹äºåˆ†ç±»é—®é¢˜ï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼š</p>
                <div class="math-formula">
                    <p><strong>L = -Î£â‚– yâ‚– log(Å·â‚–)</strong></p>
                    <br>
                    <p>å…¶ä¸­ï¼š</p>
                    <p>â€¢ <strong>y</strong> = çœŸå®æ ‡ç­¾ï¼ˆone-hot ç¼–ç ï¼‰</p>
                    <p>â€¢ <strong>Å·</strong> = é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ</p>
                    <p>â€¢ <strong>k</strong> = ç±»åˆ«ç´¢å¼•</p>
                </div>
            </div>

            <h3>3. åå‘ä¼ æ’­æ¨å¯¼</h3>
            <div class="algorithm-content">
                <p>ç›®æ ‡ï¼šè®¡ç®— âˆ‚L/âˆ‚Wâ‚‚ å’Œ âˆ‚L/âˆ‚Wâ‚</p>
                
                <h4>Step 1: è¾“å‡ºå±‚æ¢¯åº¦</h4>
                <div class="math-formula">
                    <p><strong>Î´â‚‚ = âˆ‚L/âˆ‚zâ‚‚ = Å· - y</strong></p>
                    <p>âˆ‚L/âˆ‚Wâ‚‚ = h Â· Î´â‚‚áµ€</p>
                    <p>âˆ‚L/âˆ‚bâ‚‚ = Î´â‚‚</p>
                </div>

                <h4>Step 2: éšè—å±‚æ¢¯åº¦</h4>
                <div class="math-formula">
                    <p><strong>Î´â‚ = (Wâ‚‚áµ€ Â· Î´â‚‚) âŠ™ Ïƒ'(zâ‚)</strong></p>
                    <p>âˆ‚L/âˆ‚Wâ‚ = x Â· Î´â‚áµ€</p>
                    <p>âˆ‚L/âˆ‚bâ‚ = Î´â‚</p>
                    <br>
                    <p>å…¶ä¸­ âŠ™ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼ˆHadamardç§¯ï¼‰</p>
                </div>

                <h4>Step 3: æƒé‡æ›´æ–°</h4>
                <div class="math-formula">
                    <p><strong>W â† W - Î· Ã— âˆ‚L/âˆ‚W</strong></p>
                    <p><strong>b â† b - Î· Ã— âˆ‚L/âˆ‚b</strong></p>
                    <br>
                    <p>å…¶ä¸­ <strong>Î·</strong> æ˜¯å­¦ä¹ ç‡</p>
                </div>
            </div>

            <h2>æ¿€æ´»å‡½æ•°å¯¹æ¯”</h2>
            <div class="activation-visual">
                <div class="activation-func">
                    <h4>Sigmoid</h4>
                    <div class="formula">Ïƒ(x) = 1/(1+e^(-x))</div>
                    <p style="font-size: 0.9em; color: #8b949e;">è¾“å‡ºèŒƒå›´ (0, 1)</p>
                    <div class="graph">
                        <div class="bar" style="height: 10px;"></div>
                        <div class="bar" style="height: 20px;"></div>
                        <div class="bar" style="height: 40px;"></div>
                        <div class="bar" style="height: 55px;"></div>
                    </div>
                </div>
                <div class="activation-func">
                    <h4>ReLU</h4>
                    <div class="formula">f(x) = max(0, x)</div>
                    <p style="font-size: 0.9em; color: #8b949e;">è¾“å‡ºèŒƒå›´ [0, +âˆ)</p>
                    <div class="graph">
                        <div class="bar" style="height: 0px;"></div>
                        <div class="bar" style="height: 20px;"></div>
                        <div class="bar" style="height: 40px;"></div>
                        <div class="bar" style="height: 60px;"></div>
                    </div>
                </div>
                <div class="activation-func">
                    <h4>Tanh</h4>
                    <div class="formula">tanh(x) = (e^x-e^(-x))/(e^x+e^(-x))</div>
                    <p style="font-size: 0.9em; color: #8b949e;">è¾“å‡ºèŒƒå›´ (-1, 1)</p>
                    <div class="graph">
                        <div class="bar" style="height: 10px;"></div>
                        <div class="bar" style="height: 30px;"></div>
                        <div class="bar" style="height: 50px;"></div>
                        <div class="bar" style="height: 55px;"></div>
                    </div>
                </div>
            </div>

            <h2>è®­ç»ƒè¿‡ç¨‹</h2>
            <div class="gradient-flow">
                <div class="gradient-step">
                    <div class="step-num">1</div>
                    <h4>åˆå§‹åŒ–</h4>
                    <p>éšæœºåˆå§‹åŒ–æƒé‡</p>
                </div>
                <div class="gradient-step">
                    <div class="step-num">2</div>
                    <h4>å‰å‘ä¼ æ’­</h4>
                    <p>è®¡ç®—ç½‘ç»œè¾“å‡º</p>
                </div>
                <div class="gradient-step">
                    <div class="step-num">3</div>
                    <h4>è®¡ç®—æŸå¤±</h4>
                    <p>L = -Î£yÂ·log(Å·)</p>
                </div>
                <div class="gradient-step">
                    <div class="step-num">4</div>
                    <h4>åå‘ä¼ æ’­</h4>
                    <p>è®¡ç®—æ¢¯åº¦ Î´</p>
                </div>
                <div class="gradient-step">
                    <div class="step-num">5</div>
                    <h4>æ›´æ–°æƒé‡</h4>
                    <p>W = W - Î·Â·âˆ‚L/âˆ‚W</p>
                </div>
                <div class="gradient-step">
                    <div class="step-num">6</div>
                    <h4>æ£€æŸ¥æ”¶æ•›</h4>
                    <p>æŸå¤±è¶³å¤Ÿå°ï¼Ÿ</p>
                </div>
            </div>

            <h2>å®Œæ•´å®ç°</h2>
            <div class="algorithm-content">
                <div class="code-tabs">
                    <div class="code-tab-header">
                        <button class="code-tab-btn active" data-tab="bp-js">JavaScript</button>
                        <button class="code-tab-btn" data-tab="bp-python">Python</button>
                        <button class="code-tab-btn" data-tab="bp-go">Go</button>
                    </div>
                    <div id="bp-js" class="code-tab-content active">
                        <div class="code-block line-numbers">
                            <pre><code class="language-javascript">
class NeuralNetwork {
    constructor(inputSize, hiddenSize, outputSize, learningRate = 0.01) {
        this.inputSize = inputSize;
        this.hiddenSize = hiddenSize;
        this.outputSize = outputSize;
        this.learningRate = learningRate;
        
        // Xavier åˆå§‹åŒ–
        this.W1 = this.randomMatrix(inputSize, hiddenSize);
        this.b1 = new Array(hiddenSize).fill(0);
        this.W2 = this.randomMatrix(hiddenSize, outputSize);
        this.b2 = new Array(outputSize).fill(0);
    }
    
    randomMatrix(rows, cols) {
        const scale = Math.sqrt(2.0 / rows);
        return Array.from({length: rows}, () => 
            Array.from({length: cols}, () => (Math.random() * 2 - 1) * scale));
    }
    
    sigmoid(x) {
        return 1 / (1 + Math.exp(-Math.max(-500, Math.min(500, x))));
    }
    
    sigmoidDerivative(x) {
        const s = this.sigmoid(x);
        return s * (1 - s);
    }
    
    softmax(arr) {
        const maxVal = Math.max(...arr);
        const exps = arr.map(x => Math.exp(x - maxVal));
        const sumExps = exps.reduce((a, b) => a + b, 0);
        return exps.map(x => x / sumExps);
    }
    
    // å‰å‘ä¼ æ’­
    forward(inputs) {
        // éšè—å±‚
        this.z1 = new Array(this.hiddenSize).fill(0);
        for (let j = 0; j < this.hiddenSize; j++) {
            for (let i = 0; i < this.inputSize; i++) {
                this.z1[j] += inputs[i] * this.W1[i][j];
            }
            this.z1[j] += this.b1[j];
        }
        this.hidden = this.z1.map(x => this.sigmoid(x));
        
        // è¾“å‡ºå±‚
        this.z2 = new Array(this.outputSize).fill(0);
        for (let k = 0; k < this.outputSize; k++) {
            for (let j = 0; j < this.hiddenSize; j++) {
                this.z2[k] += this.hidden[j] * this.W2[j][k];
            }
            this.z2[k] += this.b2[k];
        }
        this.output = this.softmax(this.z2);
        
        return this.output;
    }
    
    // åå‘ä¼ æ’­
    backward(inputs, target) {
        const outputSize = this.outputSize;
        const hiddenSize = this.hiddenSize;
        const inputSize = this.inputSize;
        
        // è¾“å‡ºå±‚è¯¯å·®
        const delta2 = new Array(outputSize);
        for (let k = 0; k < outputSize; k++) {
            delta2[k] = this.output[k] - target[k];
        }
        
        // æ›´æ–° W2 å’Œ b2
        for (let j = 0; j < hiddenSize; j++) {
            for (let k = 0; k < outputSize; k++) {
                const gradient = this.learningRate * delta2[k] * this.hidden[j];
                this.W2[j][k] -= gradient;
            }
        }
        for (let k = 0; k < outputSize; k++) {
            this.b2[k] -= this.learningRate * delta2[k];
        }
        
        // éšè—å±‚è¯¯å·®
        const delta1 = new Array(hiddenSize);
        for (let j = 0; j < hiddenSize; j++) {
            let error = 0;
            for (let k = 0; k < outputSize; k++) {
                error += delta2[k] * this.W2[j][k];
            }
            delta1[j] = error * this.sigmoidDerivative(this.z1[j]);
        }
        
        // æ›´æ–° W1 å’Œ b1
        for (let i = 0; i < inputSize; i++) {
            for (let j = 0; j < hiddenSize; j++) {
                const gradient = this.learningRate * delta1[j] * inputs[i];
                this.W1[i][j] -= gradient;
            }
        }
        for (let j = 0; j < hiddenSize; j++) {
            this.b1[j] -= this.learningRate * delta1[j];
        }
    }
    
    // è®­ç»ƒä¸€ä¸ª epoch
    train(X, y, epochs = 100) {
        const losses = [];
        for (let epoch = 0; epoch < epochs; epoch++) {
            let totalLoss = 0;
            
            for (let i = 0; i < X.length; i++) {
                const output = this.forward(X[i]);
                this.backward(X[i], y[i]);
                
                // è®¡ç®—äº¤å‰ç†µæŸå¤±
                const sampleLoss = -y[i].reduce((sum, yk, k) => {
                    return sum + (yk > 0 ? yk * Math.log(output[k] + 1e-10) : 0);
                }, 0);
                totalLoss += sampleLoss;
            }
            
            const avgLoss = totalLoss / X.length;
            losses.push(avgLoss);
            
            if (epoch % 10 === 0) {
                console.log(`Epoch ${epoch}: Loss = ${avgLoss.toFixed(4)}`);
            }
        }
        
        return losses;
    }
    
    // é¢„æµ‹
    predict(inputs) {
        return this.forward(inputs);
    }
    
    // å‡†ç¡®ç‡
    accuracy(X, y) {
        let correct = 0;
        for (let i = 0; i < X.length; i++) {
            const output = this.predict(X[i]);
            const predicted = output.indexOf(Math.max(...output));
            const actual = y[i].indexOf(1);
            if (predicted === actual) correct++;
        }
        return correct / X.length;
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const nn = new NeuralNetwork(2, 4, 2, 0.5);

// è®­ç»ƒæ•°æ®ï¼šXOR é—®é¢˜
const X = [
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
];
const y = [
    [1, 0],  // 0 XOR 0 = 0
    [0, 1],  // 0 XOR 1 = 1
    [0, 1],  // 1 XOR 0 = 1
    [1, 0]   // 1 XOR 1 = 0
];

console.log("Training XOR problem...");
const losses = nn.train(X, y, 1000);

console.log("\nTest results:");
X.forEach((input, i) => {
    const output = nn.predict(input);
    const predicted = output.indexOf(Math.max(...output));
    console.log(`${input[0]} XOR ${input[1]} = ${predicted}`);
});

console.log(`Accuracy: ${(nn.accuracy(X, y) * 100).toFixed(0)}%`);
                            </code></pre>
                        </div>
                    </div>
                    <div id="bp-python" class="code-tab-content">
                        <div class="code-block line-numbers">
                            <pre><code class="language-python">
import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # Xavier åˆå§‹åŒ–
        scale = np.sqrt(2.0 / input_size)
        self.W1 = np.random.randn(input_size, hidden_size) * scale
        self.b1 = np.zeros(hidden_size)
        self.W2 = np.random.randn(hidden_size, output_size) * scale
        self.b2 = np.zeros(output_size)
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_derivative(self, x):
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x))
        return exp_x / np.sum(exp_x)
    
    def forward(self, inputs):
        # éšè—å±‚
        self.z1 = np.dot(inputs, self.W1) + self.b1
        self.hidden = self.sigmoid(self.z1)
        
        # è¾“å‡ºå±‚
        self.z2 = np.dot(self.hidden, self.W2) + self.b2
        self.output = self.softmax(self.z2)
        
        return self.output
    
    def backward(self, inputs, target):
        # è¾“å‡ºå±‚è¯¯å·®
        delta2 = self.output - target
        
        # æ›´æ–° W2 å’Œ b2
        self.W2 -= self.learning_rate * np.outer(self.hidden, delta2)
        self.b2 -= self.learning_rate * delta2
        
        # éšè—å±‚è¯¯å·®
        delta1 = np.dot(delta2, self.W2.T) * self.sigmoid_derivative(self.z1)
        
        # æ›´æ–° W1 å’Œ b1
        self.W1 -= self.learning_rate * np.outer(inputs, delta1)
        self.b1 -= self.learning_rate * delta1
    
    def train(self, X, y, epochs=100):
        losses = []
        for epoch in range(epochs):
            total_loss = 0
            
            for i in range(len(X)):
                output = self.forward(X[i])
                self.backward(X[i], y[i])
                
                # äº¤å‰ç†µæŸå¤±
                sample_loss = -np.sum(y[i] * np.log(output + 1e-10))
                total_loss += sample_loss
            
            avg_loss = total_loss / len(X)
            losses.append(avg_loss)
            
            if epoch % 10 == 0:
                print(f'Epoch {epoch}: Loss = {avg_loss:.4f}')
        
        return losses
    
    def predict(self, inputs):
        return self.forward(inputs)
    
    def accuracy(self, X, y):
        correct = 0
        for i in range(len(X)):
            output = self.predict(X[i])
            predicted = np.argmax(output)
            actual = np.argmax(y[i])
            if predicted == actual:
                correct += 1
        return correct / len(X)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=2, learning_rate=0.5)
    
    # è®­ç»ƒæ•°æ®ï¼šXOR é—®é¢˜
    X = np.array([
        [0, 0],
        [0, 1],
        [1, 0],
        [1, 1]
    ])
    y = np.array([
        [1, 0],  # 0 XOR 0 = 0
        [0, 1],  # 0 XOR 1 = 1
        [0, 1],  # 1 XOR 0 = 1
        [1, 0]   # 1 XOR 1 = 0
    ])
    
    print("Training XOR problem...")
    losses = nn.train(X, y, epochs=1000)
    
    print("\nTest results:")
    for input_row in X:
        output = nn.predict(input_row)
        predicted = np.argmax(output)
        print(f"{input_row[0]} XOR {input_row[1]} = {predicted}")
    
    print(f"Accuracy: {nn.accuracy(X, y) * 100:.0f}%")
                            </code></pre>
                        </div>
                    </div>
                    <div id="bp-go" class="code-tab-content">
                        <div class="code-block line-numbers">
                            <pre><code class="language-go">
package main

import (
    "fmt"
    "math"
    "math/rand"
)

type NeuralNetwork struct {
    inputSize    int
    hiddenSize   int
    outputSize   int
    learningRate float64
    W1           [][]float64
    b1           []float64
    W2           [][]float64
    b2           []float64
}

func NewNeuralNetwork(inputSize, hiddenSize, outputSize int, learningRate float64) *NeuralNetwork {
    nn := &NeuralNetwork{
        inputSize:    inputSize,
        hiddenSize:   hiddenSize,
        outputSize:   outputSize,
        learningRate: learningRate,
    }
    
    // Xavier åˆå§‹åŒ–
    scale := math.Sqrt(2.0 / float64(inputSize))
    nn.W1 = make([][]float64, inputSize)
    nn.b1 = make([]float64, hiddenSize)
    nn.W2 = make([][]float64, hiddenSize)
    nn.b2 = make([]float64, outputSize)
    
    for i := range nn.W1 {
        nn.W1[i] = make([]float64, hiddenSize)
        for j := range nn.W1[i] {
            nn.W1[i][j] = (rand.Float64()*2 - 1) * scale
        }
    }
    
    for j := range nn.W2 {
        nn.W2[j] = make([]float64, outputSize)
        for k := range nn.W2[j] {
            nn.W2[j][k] = (rand.Float64()*2 - 1) * scale
        }
    }
    
    return nn
}

func (nn *NeuralNetwork) sigmoid(x float64) float64 {
    if x > 500 {
        return 1.0
    }
    if x < -500 {
        return 0.0
    }
    return 1 / (1 + math.Exp(-x))
}

func (nn *NeuralNetwork) sigmoidDerivative(x float64) float64 {
    s := nn.sigmoid(x)
    return s * (1 - s)
}

func (nn *NeuralNetwork) softmax(z []float64) []float64 {
    maxVal := z[0]
    for _, v := range z {
        if v > maxVal {
            maxVal = v
        }
    }
    
    expZ := make([]float64, len(z))
    sumExp := 0.0
    for i, v := range z {
        expZ[i] = math.Exp(v - maxVal)
        sumExp += expZ[i]
    }
    
    for i := range expZ {
        expZ[i] /= sumExp
    }
    
    return expZ
}

func (nn *NeuralNetwork) Forward(inputs []float64) []float64 {
    // éšè—å±‚
    z1 := make([]float64, nn.hiddenSize)
    for j := 0; j < nn.hiddenSize; j++ {
        sum := 0.0
        for i := 0; i < nn.inputSize; i++ {
            sum += inputs[i] * nn.W1[i][j]
        }
        z1[j] = sum + nn.b1[j]
    }
    
    hidden := make([]float64, nn.hiddenSize)
    for j := 0; j < nn.hiddenSize; j++ {
        hidden[j] = nn.sigmoid(z1[j])
    }
    
    // è¾“å‡ºå±‚
    z2 := make([]float64, nn.outputSize)
    for k := 0; k < nn.outputSize; k++ {
        sum := 0.0
        for j := 0; j < nn.hiddenSize; j++ {
            sum += hidden[j] * nn.W2[j][k]
        }
        z2[k] = sum + nn.b2[k]
    }
    
    output := nn.softmax(z2)
    
    return output
}

func (nn *NeuralNetwork) Backward(inputs, target []float64) {
    // è¾“å‡ºå±‚è¯¯å·®
    output := nn.Forward(inputs)
    delta2 := make([]float64, nn.outputSize)
    for k := 0; k < nn.outputSize; k++ {
        delta2[k] = output[k] - target[k]
    }
    
    // éšè—å±‚è¾“å‡º
    z1 := make([]float64, nn.hiddenSize)
    hidden := make([]float64, nn.hiddenSize)
    for j := 0; j < nn.hiddenSize; j++ {
        sum := 0.0
        for i := 0; i < nn.inputSize; i++ {
            sum += inputs[i] * nn.W1[i][j]
        }
        z1[j] = sum + nn.b1[j]
        hidden[j] = nn.sigmoid(z1[j])
    }
    
    // æ›´æ–° W2 å’Œ b2
    for j := 0; j < nn.hiddenSize; j++ {
        for k := 0; k < nn.outputSize; k++ {
            gradient := nn.learningRate * delta2[k] * hidden[j]
            nn.W2[j][k] -= gradient
        }
    }
    for k := 0; k < nn.outputSize; k++ {
        nn.b2[k] -= nn.learningRate * delta2[k]
    }
    
    // éšè—å±‚è¯¯å·®
    delta1 := make([]float64, nn.hiddenSize)
    for j := 0; j < nn.hiddenSize; j++ {
        error := 0.0
        for k := 0; k < nn.outputSize; k++ {
            error += delta2[k] * nn.W2[j][k]
        }
        delta1[j] = error * nn.sigmoidDerivative(z1[j])
    }
    
    // æ›´æ–° W1 å’Œ b1
    for i := 0; i < nn.inputSize; i++ {
        for j := 0; j < nn.hiddenSize; j++ {
            gradient := nn.learningRate * delta1[j] * inputs[i]
            nn.W1[i][j] -= gradient
        }
    }
    for j := 0; j < nn.hiddenSize; j++ {
        nn.b1[j] -= nn.learningRate * delta1[j]
    }
}

func (nn *NeuralNetwork) Train(X [][]float64, y [][]float64, epochs int) {
    for epoch := 0; epoch < epochs; epoch++ {
        for i := range X {
            nn.Backward(X[i], y[i])
        }
        
        if epoch%10 == 0 {
            loss := 0.0
            for i := range X {
                output := nn.Forward(X[i])
                for k := range output {
                    if y[i][k] > 0 {
                        loss -= y[i][k] * math.Log(output[k]+1e-10)
                    }
                }
            }
            fmt.Printf("Epoch %d: Loss = %.4f\n", epoch, loss/float64(len(X)))
        }
    }
}

func (nn *NeuralNetwork) Predict(inputs []float64) []float64 {
    return nn.Forward(inputs)
}

func main() {
    nn := NewNeuralNetwork(2, 4, 2, 0.5)
    
    // XOR é—®é¢˜
    X := [][]float64{
        {0, 0},
        {0, 1},
        {1, 0},
        {1, 1},
    }
    y := [][]float64{
        {1, 0}, // 0 XOR 0 = 0
        {0, 1}, // 0 XOR 1 = 1
        {0, 1}, // 1 XOR 0 = 1
        {1, 0}, // 1 XOR 1 = 0
    }
    
    fmt.Println("Training XOR problem...")
    nn.Train(X, y, 1000)
    
    fmt.Println("\nTest results:")
    for _, input := range X {
        output := nn.Predict(input)
        predicted := 0
        for k, v := range output {
            if v > output[predicted] {
                predicted = k
            }
        }
        fmt.Printf("%.0f XOR %.0f = %d\n", input[0], input[1], predicted)
    }
}
                            </code></pre>
                        </div>
                    </div>
                </div>
            </div>

            <h2>ä¼˜åŒ–æŠ€å·§</h2>
            <div class="algorithm-content">
                <table class="comparison-table">
                    <tr>
                        <th>æŠ€å·§</th>
                        <th>ä½œç”¨</th>
                        <th>è¯´æ˜</th>
                    </tr>
                    <tr>
                        <td>Mini-batch</td>
                        <td>åŠ é€Ÿè®­ç»ƒã€ç¨³å®šæ”¶æ•›</td>
                        <td>æ¯æ¬¡ä½¿ç”¨å¤šä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦</td>
                    </tr>
                    <tr>
                        <td>Momentum</td>
                        <td>åŠ é€Ÿæ”¶æ•›ã€å‡å°‘éœ‡è¡</td>
                        <td>å¼•å…¥å†å²æ¢¯åº¦ä¿¡æ¯</td>
                    </tr>
                    <tr>
                        <td>Learning Rate Decay</td>
                        <td>ç²¾ç»†è°ƒä¼˜</td>
                        <td>éšè®­ç»ƒé€æ¸å‡å°å­¦ä¹ ç‡</td>
                    </tr>
                    <tr>
                        <td>Dropout</td>
                        <td>é˜²æ­¢è¿‡æ‹Ÿåˆ</td>
                        <td>éšæœºä¸¢å¼ƒéƒ¨åˆ†ç¥ç»å…ƒ</td>
                    </tr>
                    <tr>
                        <td>Batch Normalization</td>
                        <td>åŠ é€Ÿè®­ç»ƒã€ç¨³å®šåˆ†å¸ƒ</td>
                        <td>æ ‡å‡†åŒ–æ¯å±‚è¾“å…¥</td>
                    </tr>
                </table>
            </div>

            <h2>å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ</h2>
            <div class="algorithm-content">
                <h3>1. æ¢¯åº¦æ¶ˆå¤± (Vanishing Gradient)</h3>
                <div class="concept-box">
                    <p><strong>é—®é¢˜</strong>ï¼šæ·±å±‚ç½‘ç»œä¸­ï¼Œæ¢¯åº¦é€å±‚æŒ‡æ•°è¡°å‡</p>
                    <p><strong>è§£å†³</strong>ï¼šä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°ã€æ®‹å·®è¿æ¥ã€æ‰¹å½’ä¸€åŒ–</p>
                </div>
                
                <h3>2. æ¢¯åº¦çˆ†ç‚¸ (Exploding Gradient)</h3>
                <div class="concept-box">
                    <p><strong>é—®é¢˜</strong>ï¼šæ¢¯åº¦é€å±‚æŒ‡æ•°å¢é•¿ï¼Œæƒé‡æ›´æ–°è¿‡å¤§</p>
                    <p><strong>è§£å†³</strong>ï¼šæ¢¯åº¦è£å‰ª (Gradient Clipping)ã€æƒé‡æ­£åˆ™åŒ–</p>
                </div>
                
                <h3>3. è¿‡æ‹Ÿåˆ (Overfitting)</h3>
                <div class="concept-box">
                    <p><strong>é—®é¢˜</strong>ï¼šè®­ç»ƒé›†è¡¨ç°å¥½ï¼Œæµ‹è¯•é›†è¡¨ç°å·®</p>
                    <p><strong>è§£å†³</strong>ï¼šDropoutã€L2æ­£åˆ™åŒ–ã€æ—©åœã€æ•°æ®å¢å¼º</p>
                </div>
            </div>

            <h2>BPç®—æ³• vs å…¶ä»–ä¼˜åŒ–æ–¹æ³•</h2>
            <div class="algorithm-content">
                <table class="comparison-table">
                    <tr>
                        <th>æ–¹æ³•</th>
                        <th>å¤æ‚åº¦</th>
                        <th>é€‚ç”¨åœºæ™¯</th>
                    </tr>
                    <tr>
                        <td>SGD</td>
                        <td>O(1) æ¯æ ·æœ¬</td>
                        <td>å¤§è§„æ¨¡æ•°æ®</td>
                    </tr>
                    <tr>
                        <td>Momentum</td>
                        <td>O(1) æ¯æ ·æœ¬</td>
                        <td>å³¡è°·åœ°å½¢</td>
                    </tr>
                    <tr>
                        <td>Adam</td>
                        <td>O(1) æ¯æ ·æœ¬</td>
                        <td>å¤§å¤šæ•°åœºæ™¯</td>
                    </tr>
                    <tr>
                        <td>L-BFGS</td>
                        <td>O(m) å†…å­˜</td>
                        <td>å°è§„æ¨¡æ•°æ®</td>
                    </tr>
                </table>
            </div>

            <h2>åº”ç”¨åœºæ™¯</h2>
            <div class="algorithm-content">
                <h3>1. å›¾åƒè¯†åˆ«</h3>
                <p>å·ç§¯ç¥ç»ç½‘ç»œ (CNN) ä½¿ç”¨ BP ç®—æ³•è®­ç»ƒï¼Œç”¨äº MNISTã€CIFARã€ImageNet ç­‰å›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚</p>
                
                <h3>2. è‡ªç„¶è¯­è¨€å¤„ç†</h3>
                <p>å¾ªç¯ç¥ç»ç½‘ç»œ (RNN) å’Œ Transformer ä½¿ç”¨ BP ç®—æ³•è®­ç»ƒï¼Œç”¨äºæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚</p>
                
                <h3>3. è¯­éŸ³è¯†åˆ«</h3>
                <p>æ·±åº¦ç¥ç»ç½‘ç»œç”¨äºå£°å­¦æ¨¡å‹è®­ç»ƒï¼Œå°†å£°å­¦ç‰¹å¾è½¬æ¢ä¸ºæ–‡æœ¬ã€‚</p>
                
                <h3>4. æ¨èç³»ç»Ÿ</h3>
                <p>æ·±åº¦å­¦ä¹ æ¨¡å‹å­¦ä¹ ç”¨æˆ·å’Œç‰©å“çš„è¡¨ç¤ºï¼Œç”¨äºä¸ªæ€§åŒ–æ¨èã€‚</p>
                
                <h3>5. æ¸¸æˆAI</h3>
                <p>AlphaGo ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå’Œå¼ºåŒ–å­¦ä¹ ï¼Œå…¶ä¸­ç­–ç•¥ç½‘ç»œä½¿ç”¨ BP ç®—æ³•è®­ç»ƒã€‚</p>
            </div>

            <h2>æ€»ç»“ï¼šBPç®—æ³•çš„æ ¸å¿ƒæ´å¯Ÿ</h2>
            <div class="concept-box">
                <h4>å…³é”®åˆ›æ–°</h4>
                <ol>
                    <li><strong>é“¾å¼æ³•åˆ™</strong>ï¼šé«˜æ•ˆè®¡ç®—å¤åˆå‡½æ•°çš„æ¢¯åº¦</li>
                    <li><strong>åå‘ä¼ æ’­</strong>ï¼šå°†è¯¯å·®ä»è¾“å‡ºå±‚ä¼ é€’åˆ°è¾“å…¥å±‚</li>
                    <li><strong>æ¢¯åº¦ä¸‹é™</strong>ï¼šé€šè¿‡è¿­ä»£ä¼˜åŒ–æ‰¾åˆ°æœ€ä¼˜æƒé‡</li>
                    <li><strong>è‡ªåŠ¨å¾®åˆ†</strong>ï¼šæ— éœ€æ‰‹åŠ¨æ±‚å¯¼ï¼Œç”±è®¡ç®—å›¾è‡ªåŠ¨å®Œæˆ</li>
                </ol>
            </div>

            <h3>å­¦ä¹ å»ºè®®</h3>
            <div class="algorithm-content">
                <ul>
                    <li>æŒæ¡å¾®ç§¯åˆ†çš„é“¾å¼æ³•åˆ™å’Œåå¯¼æ•°æ¦‚å¿µ</li>
                    <li>æ‰‹åŠ¨æ¨å¯¼ 2-3 å±‚ç½‘ç»œçš„ BP å…¬å¼</li>
                    <li>ä½¿ç”¨ NumPy ä»é›¶å®ç° BP ç®—æ³•</li>
                    <li>å­¦ä¹ æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸çš„åŸå› å’Œè§£å†³æ–¹æ³•</li>
                    <li>äº†è§£ç°ä»£ä¼˜åŒ–å™¨ï¼šMomentumã€Adamã€RMSprop</li>
                    <li>å­¦ä¹  PyTorch æˆ– TensorFlow çš„è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶</li>
                </ul>
            </div>
        </div>
    </section>

    <script src="../js/main.js"></script>
    <script src="../lib/prism/prism.min.js"></script>
    <script src="../lib/prism/prism-python.min.js"></script>
    <script src="../lib/prism/prism-go.min.js"></script>
    <script src="../lib/prism/prism-line-numbers.min.js"></script>
</body>
</html>
