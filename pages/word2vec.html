<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Word2Vec - ç®—æ³•å­¦ä¹ æŒ‡å—</title>
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../lib/prism/prism-tomorrow.min.css">
    <link rel="stylesheet" href="../lib/prism/prism-line-numbers.min.css">
    <style>
        .concept-box {
            background: #f8f9fa;
            border-left: 4px solid #3498db;
            padding: 20px;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }
        
        .architecture-diagram {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            flex-wrap: wrap;
            margin: 30px 0;
            padding: 30px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border-radius: 15px;
        }
        
        .layer {
            background: white;
            padding: 20px 30px;
            border-radius: 10px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(0,0,0,0.2);
            min-width: 120px;
        }
        
        .layer h4 {
            color: #2c3e50;
            margin-bottom: 10px;
        }
        
        .arrow-right {
            font-size: 30px;
            color: white;
        }
        
        .vector-visual {
            display: flex;
            justify-content: center;
            gap: 5px;
            margin: 20px 0;
        }
        
        .vector-dim {
            width: 40px;
            height: 80px;
            background: linear-gradient(180deg, #3498db, #2980b9);
            border-radius: 5px;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
            font-size: 12px;
        }
        
        .embedding-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(100px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        
        .word-vector {
            background: #0d1117;
            color: #c9d1d9;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
            border: 1px solid #30363d;
        }
        
        .word-vector .word {
            font-size: 1.2em;
            font-weight: bold;
            color: #58a6ff;
            margin-bottom: 8px;
        }
        
        .word-vector .vector {
            font-family: monospace;
            font-size: 0.8em;
            color: #8b949e;
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        .comparison-table th,
        .comparison-table td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        
        .comparison-table th {
            background: #3498db;
            color: white;
        }
        
        .comparison-table tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        .highlight-text {
            background: #fff3cd;
            padding: 2px 6px;
            border-radius: 3px;
        }
        
        .math-formula {
            background: #f8f9fa;
            border-left: 4px solid #3498db;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Times New Roman', serif;
            font-size: 1.1em;
            border-radius: 0 8px 8px 0;
        }
        
        .training-visual {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 30px;
            flex-wrap: wrap;
            margin: 30px 0;
            padding: 30px;
            background: #f0f4f8;
            border-radius: 10px;
        }
        
        .training-step {
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            text-align: center;
            min-width: 150px;
        }
        
        .training-step .step-num {
            background: #3498db;
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 0 auto 10px;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <nav class="navbar">
        <div class="container">
            <h1 class="logo"></h1>
            <ul class="nav-links">
                <li><a href="../index.html">é¦–é¡µ</a></li>
                <li><a href="../index.html#algorithms">ç®—æ³•åˆ†ç±»</a></li>
                <li><a href="../index.html#practice">åœ¨çº¿ç»ƒä¹ </a></li>
            </ul>
        </div>
    </nav>

    <section class="algorithm-detail">
        <div class="container">
            <a href="../index.html#algorithms" class="back-link">â† è¿”å›ç®—æ³•åˆ†ç±»</a>
            <h2>Word2Vec - è¯å‘é‡è¡¨ç¤º</h2>
            
            <div class="algorithm-content">
                <h3>ä»€ä¹ˆæ˜¯Word2Vecï¼Ÿ</h3>
                <p>
                    <strong>Word2Vec</strong> æ˜¯ç”± Google åœ¨ 2013 å¹´æå‡ºçš„è¯åµŒå…¥ï¼ˆWord Embeddingï¼‰æŠ€æœ¯ï¼Œ
                    å®ƒå°†è¯è¯­æ˜ å°„åˆ°ä½ç»´ç¨ å¯†å‘é‡ç©ºé—´ï¼Œä½¿å¾—è¯­ä¹‰ç›¸è¿‘çš„è¯è¯­åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»ä¹Ÿç›¸è¿‘ã€‚
                </p>
                <p>
                    Word2Vec çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š<span class="highlight-text">ä¸€ä¸ªè¯è¯­çš„æ„ä¹‰å¯ä»¥ç”±å®ƒçš„ä¸Šä¸‹æ–‡æ¥å®šä¹‰</span>ã€‚
                    "ä½ å¯ä»¥é€šè¿‡ä¸€ä¸ªè¯çš„æœ‹å‹ï¼ˆä¸Šä¸‹æ–‡ï¼‰æ¥åˆ¤æ–­å®ƒçš„èº«ä»½"ã€‚
                </p>
            </div>

            <div class="complexity">
                <div class="complexity-item">
                    <h4>å‘é‡ç»´åº¦</h4>
                    <p>é€šå¸¸ 100-300 ç»´</p>
                </div>
                <div class="complexity-item">
                    <h4>è®­ç»ƒå¤æ‚åº¦</h4>
                    <p>O(E Ã— log V)</p>
                </div>
                <div class="complexity-item">
                    <h4>æ¨ç†é€Ÿåº¦</h4>
                    <p>O(1) - å‘é‡è¿ç®—</p>
                </div>
            </div>

            <h2>æ ¸å¿ƒæ€æƒ³ï¼šåˆ†å¸ƒå¼è¡¨ç¤º</h2>
            
            <div class="concept-box">
                <h4>ğŸ¯ åˆ†å¸ƒå¼è¡¨ç¤º vs ç‹¬çƒ­ç¼–ç </h4>
                <p>
                    <strong>ä¼ ç»Ÿç‹¬çƒ­ç¼–ç </strong>ï¼šæ¯ä¸ªè¯ç”¨é«˜ç»´ç¨€ç–å‘é‡è¡¨ç¤ºï¼Œè¯ä¸è¯ä¹‹é—´å®Œå…¨ç‹¬ç«‹
                </p>
                <p>
                    <strong>Word2Vecåˆ†å¸ƒå¼è¡¨ç¤º</strong>ï¼šæ¯ä¸ªè¯ç”¨ä½ç»´ç¨ å¯†å‘é‡è¡¨ç¤ºï¼Œç›¸ä¼¼è¯è·ç¦»æ›´è¿‘
                </p>
            </div>

            <h3>è¯å‘é‡ç¤ºä¾‹</h3>
            <div class="embedding-grid">
                <div class="word-vector">
                    <div class="word">king</div>
                    <div class="vector">[0.5, 0.2, -0.1, ...]</div>
                </div>
                <div class="word-vector">
                    <div class="word">queen</div>
                    <div class="vector">[0.4, 0.3, -0.2, ...]</div>
                </div>
                <div class="word-vector">
                    <div class="word">man</div>
                    <div class="vector">[0.6, 0.1, 0.0, ...]</div>
                </div>
                <div class="word-vector">
                    <div class="word">woman</div>
                    <div class="vector">[0.5, 0.2, 0.1, ...]</div>
                </div>
            </div>
            <p style="text-align: center; color: #7f8c8d;">
                å¯ä»¥é€šè¿‡å‘é‡è¿ç®—å‘ç°è¯­ä¹‰å…³ç³»ï¼šking - man + woman â‰ˆ queen
            </p>

            <h2>ä¸¤ç§æ¨¡å‹æ¶æ„</h2>

            <h3>1. CBOW (Continuous Bag-of-Words)</h3>
            <div class="algorithm-content">
                <p>
                    <strong>åŸç†</strong>ï¼šåˆ©ç”¨ä¸Šä¸‹æ–‡è¯è¯­æ¥é¢„æµ‹å½“å‰è¯è¯­
                </p>
                <p>
                    è¾“å…¥ï¼šå‘¨å›´è¯çš„å‘é‡ â†’ ç¥ç»ç½‘ç»œ â†’ è¾“å‡ºï¼šç›®æ ‡è¯çš„é¢„æµ‹æ¦‚ç‡
                </p>
                <div class="architecture-diagram">
                    <div class="layer">
                        <h4>è¾“å…¥å±‚</h4>
                        <p>ä¸Šä¸‹æ–‡è¯å‘é‡</p>
                        <p style="color: #7f8c8d; font-size: 0.9em;">(Cä¸ªè¯çš„ç‹¬çƒ­ç¼–ç )</p>
                    </div>
                    <div class="arrow-right">â†’</div>
                    <div class="layer">
                        <h4>æŠ•å½±å±‚</h4>
                        <p>æ±‚å’Œå¹³å‡</p>
                        <p style="color: #7f8c8d; font-size: 0.9em;">(Nç»´ç¨ å¯†å‘é‡)</p>
                    </div>
                    <div class="arrow-right">â†’</div>
                    <div class="layer">
                        <h4>è¾“å‡ºå±‚</h4>
                        <p>Softmaxåˆ†ç±»</p>
                        <p style="color: #7f8c8d; font-size: 0.9em;">(é¢„æµ‹ç›®æ ‡è¯)</p>
                    </div>
                </div>
                <p style="text-align: center;">
                    <span class="highlight-text">é€‚ç”¨åœºæ™¯</span>ï¼šå°å‹è¯­æ–™åº“ï¼Œé¢‘ç¹è¯
                </p>
            </div>

            <h3>2. Skip-gram</h3>
            <div class="algorithm-content">
                <p>
                    <strong>åŸç†</strong>ï¼šåˆ©ç”¨å½“å‰è¯è¯­æ¥é¢„æµ‹å‘¨å›´çš„ä¸Šä¸‹æ–‡è¯è¯­
                </p>
                <p>
                    è¾“å…¥ï¼šå½“å‰è¯çš„å‘é‡ â†’ ç¥ç»ç½‘ç»œ â†’ è¾“å‡ºï¼šä¸Šä¸‹æ–‡è¯çš„é¢„æµ‹æ¦‚ç‡
                </p>
                <div class="architecture-diagram">
                    <div class="layer">
                        <h4>è¾“å…¥å±‚</h4>
                        <p>å½“å‰è¯å‘é‡</p>
                        <p style="color: #7f8c8d; font-size: 0.9em;">(è¯çš„ç‹¬çƒ­ç¼–ç )</p>
                    </div>
                    <div class="arrow-right">â†’</div>
                    <div class="layer">
                        <h4>æŠ•å½±å±‚</h4>
                        <p>çº¿æ€§å˜æ¢</p>
                        <p style="color: #7f8c8d; font-size: 0.9em;">(Nç»´ç¨ å¯†å‘é‡)</p>
                    </div>
                    <div class="arrow-right">â†’</div>
                    <div class="layer">
                        <h4>è¾“å‡ºå±‚</h4>
                        <p>Softmaxåˆ†ç±»</p>
                        <p style="color: #7f8c8d; font-size: 0.9em;">(é¢„æµ‹Cä¸ªä¸Šä¸‹æ–‡è¯)</p>
                    </div>
                </div>
                <p style="text-align: center;">
                    <span class="highlight-text">é€‚ç”¨åœºæ™¯</span>ï¼šå¤§å‹è¯­æ–™åº“ï¼Œç”Ÿåƒ»è¯
                </p>
            </div>

            <h2>æ•°å­¦åŸç†è¯¦è§£</h2>

            <h3>1. ç›®æ ‡å‡½æ•°</h3>
            <div class="algorithm-content">
                <p>Skip-gram çš„ç›®æ ‡å‡½æ•°ï¼šæœ€å¤§åŒ–è¯­æ–™åº“ä¸­æ‰€æœ‰çª—å£çš„è¯å¯¹å…±ç°æ¦‚ç‡</p>
                <div class="math-formula">
                    <p><strong>J(Î¸) = -1/T Î£â‚œâ‚Œ1áµ€ Î£â‚‹â‚˜â‰¤jâ‰¤m, jâ‰ 0 log P(wâ‚œâ‚Šâ±¼ | wâ‚œ)</strong></p>
                    <br>
                    <p>å…¶ä¸­ï¼š</p>
                    <p>â€¢ <strong>T</strong> = è¯­æ–™åº“å¤§å°</p>
                    <p>â€¢ <strong>m</strong> = çª—å£å¤§å°</p>
                    <p>â€¢ <strong>wâ‚œ</strong> = ä¸­å¿ƒè¯</p>
                    <p>â€¢ <strong>P(wâ‚œâ‚Šâ±¼ | wâ‚œ)</strong> = ç»™å®šä¸­å¿ƒè¯ï¼Œä¸Šä¸‹æ–‡è¯çš„æ¡ä»¶æ¦‚ç‡</p>
                </div>
            </div>

            <h3>2. æ¡ä»¶æ¦‚ç‡å‡½æ•°</h3>
            <div class="algorithm-content">
                <p>ä½¿ç”¨ softmax å‡½æ•°å®šä¹‰æ¡ä»¶æ¦‚ç‡ï¼š</p>
                <div class="math-formula">
                    <p><strong>P(w_O | w_I) = exp(v'â‚’ Â· váµ¢) / Î£â‚–â‚Œ1áµ› exp(v'â‚– Â· váµ¢)</strong></p>
                    <br>
                    <p>å…¶ä¸­ï¼š</p>
                    <p>â€¢ <strong>váµ¢</strong> = è¾“å…¥è¯å‘é‡ (center word)</p>
                    <p>â€¢ <strong>v'â‚’</strong> = è¾“å‡ºè¯å‘é‡ (context word)</p>
                    <p>â€¢ <strong>V</strong> = è¯æ±‡è¡¨å¤§å°</p>
                </div>
                <p style="color: #e74c3c; margin-top: 10px;">
                    âš ï¸ åŸå§‹ softmax è®¡ç®—å¤æ‚åº¦ä¸º O(V)ï¼Œå¯¹äºå¤§è¯æ±‡è¡¨ä¸å®ç”¨
                </p>
            </div>

            <h3>3. ä¼˜åŒ–æŠ€æœ¯ï¼šè´Ÿé‡‡æ ·</h3>
            <div class="algorithm-content">
                <p>
                    <strong>è´Ÿé‡‡æ · (Negative Sampling)</strong> æ˜¯ Word2Vec çš„å…³é”®ä¼˜åŒ–ï¼Œ
                    å°†å¤šåˆ†ç±»é—®é¢˜è½¬åŒ–ä¸ºäºŒåˆ†ç±»é—®é¢˜ã€‚
                </p>
                <div class="math-formula">
                    <p><strong>ç›®æ ‡</strong>ï¼šæœ€å¤§åŒ–æ­£æ ·æœ¬æ¦‚ç‡ï¼Œæœ€å°åŒ–è´Ÿæ ·æœ¬æ¦‚ç‡</p>
                    <br>
                    <p><strong>è´Ÿæ ·æœ¬ç›®æ ‡å‡½æ•°</strong>ï¼š</p>
                    <p>log Ïƒ(v'â‚’ Â· váµ¢) + Î£â‚–â‚Œ1á´· Eâ‚–â‚‹[log Ïƒ(-v'â‚– Â· váµ¢)]</p>
                    <br>
                    <p>å…¶ä¸­ï¼š</p>
                    <p>â€¢ <strong>Ïƒ(x) = 1/(1+e^(-x))</strong> = sigmoid å‡½æ•°</p>
                    <p>â€¢ <strong>K</strong> = è´Ÿæ ·æœ¬æ•°é‡ (é€šå¸¸5-20)</p>
                </div>
            </div>

            <h2>è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–</h2>
            <div class="training-visual">
                <div class="training-step">
                    <div class="step-num">1</div>
                    <h4>åˆå§‹åŒ–</h4>
                    <p>éšæœºåˆå§‹åŒ–è¯å‘é‡</p>
                </div>
                <div class="training-step">
                    <div class="step-num">2</div>
                    <h4>é‡‡æ ·</h4>
                    <p>é€‰æ‹©æ­£æ ·æœ¬è¯å¯¹</p>
                </div>
                <div class="training-step">
                    <div class="step-num">3</div>
                    <h4>è´Ÿé‡‡æ ·</h4>
                    <p>éšæœºé€‰æ‹©è´Ÿæ ·æœ¬</p>
                </div>
                <div class="training-step">
                    <div class="step-num">4</div>
                    <h4>æ›´æ–°</h4>
                    <p>æ¢¯åº¦ä¸‹é™ä¼˜åŒ–</p>
                </div>
                <div class="training-step">
                    <div class="step-num">5</div>
                    <h4>æ”¶æ•›</h4>
                    <p>è·å¾—ä¼˜è´¨è¯å‘é‡</p>
                </div>
            </div>

            <h2>å®Œæ•´å®ç°</h2>
            <div class="algorithm-content">
                <div class="code-tabs">
                    <div class="code-tab-header">
                        <button class="code-tab-btn active" data-tab="w2v-js">JavaScript</button>
                        <button class="code-tab-btn" data-tab="w2v-python">Python</button>
                        <button class="code-tab-btn" data-tab="w2v-go">Go</button>
                    </div>
                    <div id="w2v-js" class="code-tab-content active">
                        <div class="code-block line-numbers">
                            <pre><code class="language-javascript">
class Word2Vec {
    constructor(vocabSize = 10000, embeddingDim = 100, learningRate = 0.025, windowSize = 5) {
        this.vocabSize = vocabSize;
        this.embeddingDim = embeddingDim;
        this.learningRate = learningRate;
        this.windowSize = windowSize;
        
        // åˆå§‹åŒ–è¯å‘é‡çŸ©é˜µ (éšæœºå€¼)
        this.W = this.randomMatrix(vocabSize, embeddingDim);
        this.W_prime = this.randomMatrix(vocabSize, embeddingDim);
    }
    
    randomMatrix(rows, cols) {
        return Array.from({length: rows}, () => 
            Array.from({length: cols}, () => (Math.random() * 2 - 1) / Math.sqrt(embeddingDim)));
    }
    
    sigmoid(x) {
        return 1 / (1 + Math.exp(-Math.max(-500, Math.min(500, x))));
    }
    
    // è®­ç»ƒä¸€ä¸ª epoch
    train(sentences, epochs = 5) {
        const wordCounts = new Map();
        
        // ç»Ÿè®¡è¯é¢‘
        sentences.forEach(sentence => {
            sentence.forEach(word => {
                wordCounts.set(word, (wordCounts.get(word) || 0) + 1);
            });
        });
        
        // æ„å»ºè¯æ±‡è¡¨ï¼ˆç®€åŒ–ï¼šä½¿ç”¨åŸå§‹è¯ç´¢å¼•ï¼‰
        for (let epoch = 0; epoch < epochs; epoch++) {
            sentences.forEach(sentence => {
                for (let i = 0; i < sentence.length; i++) {
                    const centerWord = sentence[i];
                    
                    for (let j = Math.max(0, i - this.windowSize); 
                         j < Math.min(sentence.length, i + this.windowSize + 1); j++) {
                        if (i !== j) {
                            const contextWord = sentence[j];
                            this.updateVectors(centerWord, contextWord);
                        }
                    }
                }
            });
            
            console.log(`Epoch ${epoch + 1} completed`);
        }
    }
    
    updateVectors(centerWord, contextWord) {
        // ç®€åŒ–ç‰ˆè´Ÿé‡‡æ ·
        const positiveSample = [contextWord];
        const negativeSamples = [];
        
        // éšæœºé€‰æ‹©è´Ÿæ ·æœ¬
        for (let k = 0; k < 5; k++) {
            const negWord = Math.floor(Math.random() * this.vocabSize);
            if (negWord !== centerWord && negWord !== contextWord) {
                negativeSamples.push(negWord);
            }
        }
        
        // æ›´æ–°æ­£æ ·æœ¬
        this.trainSample(centerWord, contextWord, 1);
        
        // æ›´æ–°è´Ÿæ ·æœ¬
        negativeSamples.forEach(negWord => {
            this.trainSample(centerWord, negWord, -1);
        });
    }
    
    trainSample(centerWord, targetWord, label) {
        const h = this.W[centerWord];  // éšå±‚å‘é‡
        const v = this.W_prime[targetWord];  // è¾“å‡ºå‘é‡
        
        const score = this.sigmoid(this.dot(h, v));
        const error = label - score;
        
        // æ›´æ–°å‘é‡
        for (let i = 0; i < this.embeddingDim; i++) {
            const gradient = this.learningRate * error;
            h[i] += gradient * v[i];
            v[i] += gradient * h[i];
        }
        
        this.W[centerWord] = h;
        this.W_prime[targetWord] = v;
    }
    
    dot(vec1, vec2) {
        return vec1.reduce((sum, val, i) => sum + val * vec2[i], 0);
    }
    
    // è·å–è¯å‘é‡
    getVector(word) {
        return this.W[word];
    }
    
    // è®¡ç®—ç›¸ä¼¼åº¦
    cosineSimilarity(word1, word2) {
        const v1 = this.getVector(word1);
        const v2 = this.getVector(word2);
        
        if (!v1 || !v2) return null;
        
        const dot = this.dot(v1, v2);
        const mag1 = Math.sqrt(this.dot(v1, v1));
        const mag2 = Math.sqrt(this.dot(v2, v2));
        
        return dot / (mag1 * mag2);
    }
    
    // æ‰¾æœ€ç›¸ä¼¼çš„è¯
    mostSimilar(word, topN = 10) {
        const wordVec = this.getVector(word);
        if (!wordVec) return [];
        
        const similarities = [];
        for (let i = 0; i < this.vocabSize; i++) {
            if (i !== word) {
                const sim = this.cosineSimilarity(word, i);
                similarities.push({word: i, similarity: sim});
            }
        }
        
        return similarities
            .filter(s => s.similarity !== null)
            .sort((a, b) => b.similarity - a.similarity)
            .slice(0, topN);
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const sentences = [
    ['the', 'cat', 'sat', 'on', 'the', 'mat'],
    ['the', 'dog', 'ran', 'to', 'the', 'park'],
    ['cats', 'and', 'dogs', 'are', 'animals'],
    ['a', 'cat', 'is', 'a', 'pet', 'animal']
];

const w2v = new Word2Vec(100, 50, 0.025, 2);
w2v.train(sentences, 10);

console.log('Word2Vec model trained!');
                            </code></pre>
                        </div>
                    </div>
                    <div id="w2v-python" class="code-tab-content">
                        <div class="code-block line-numbers">
                            <pre><code class="language-python">
import numpy as np
from collections import defaultdict
import random

class Word2Vec:
    def __init__(self, vocab_size=10000, embedding_dim=100, learning_rate=0.025, window_size=5):
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.learning_rate = learning_rate
        self.window_size = window_size
        
        # åˆå§‹åŒ–è¯å‘é‡çŸ©é˜µ (Xavieråˆå§‹åŒ–)
        self.W = np.random.randn(vocab_size, embedding_dim) / np.sqrt(embedding_dim)
        self.W_prime = np.random.randn(vocab_size, embedding_dim) / np.sqrt(embedding_dim)
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def train(self, sentences, epochs=5):
        """è®­ç»ƒæ¨¡å‹"""
        for epoch in range(epochs):
            for sentence in sentences:
                for i, center_word in enumerate(sentence):
                    for j, context_word in enumerate(sentence):
                        if i != j:
                            self.update_vectors(center_word, context_word)
            print(f'Epoch {epoch + 1} completed')
    
    def update_vectors(self, center_word, context_word):
        """æ›´æ–°å‘é‡ï¼ˆè´Ÿé‡‡æ ·ï¼‰"""
        # æ­£æ ·æœ¬
        self.train_sample(center_word, context_word, 1)
        
        # è´Ÿæ ·æœ¬
        for _ in range(5):
            neg_word = random.randint(0, self.vocab_size - 1)
            if neg_word != center_word and neg_word != context_word:
                self.train_sample(center_word, neg_word, -1)
    
    def train_sample(self, center_word, target_word, label):
        """è®­ç»ƒå•ä¸ªæ ·æœ¬"""
        h = self.W[center_word]  # éšå±‚å‘é‡
        v = self.W_prime[target_word]  # è¾“å‡ºå‘é‡
        
        score = self.sigmoid(np.dot(h, v))
        error = label - score
        
        # æ›´æ–°å‘é‡
        gradient = self.learning_rate * error
        self.W[center_word] += gradient * v
        self.W_prime[target_word] += gradient * h
    
    def get_vector(self, word):
        """è·å–è¯å‘é‡"""
        return self.W[word]
    
    def cosine_similarity(self, word1, word2):
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        v1 = self.get_vector(word1)
        v2 = self.get_vector(word2)
        
        if v1 is None or v2 is None:
            return None
        
        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
    
    def most_similar(self, word, top_n=10):
        """æ‰¾æœ€ç›¸ä¼¼çš„è¯"""
        word_vec = self.get_vector(word)
        if word_vec is None:
            return []
        
        similarities = []
        for i in range(self.vocab_size):
            if i != word:
                sim = self.cosine_similarity(word, i)
                if sim is not None:
                    similarities.append((i, sim))
        
        return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]
    
    def vector_math(self, positive, negative, top_n=5):
        """å‘é‡è¿ç®—: king - man + woman â‰ˆ queen"""
        result_vec = np.zeros(self.embedding_dim)
        
        for word in positive:
            result_vec += self.get_vector(word)
        
        for word in negative:
            result_vec -= self.get_vector(word)
        
        similarities = []
        for i in range(self.vocab_size):
            if i not in positive and i not in negative:
                sim = np.dot(result_vec, self.get_vector(i)) / (
                    np.linalg.norm(result_vec) * np.linalg.norm(self.get_vector(i))
                )
                similarities.append((i, sim))
        
        return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    sentences = [
        ['the', 'cat', 'sat', 'on', 'the', 'mat'],
        ['the', 'dog', 'ran', 'to', 'the', 'park'],
        ['cats', 'and', 'dogs', 'are', 'animals'],
        ['a', 'cat', 'is', 'a', 'pet', 'animal']
    ]
    
    w2v = Word2Vec(vocab_size=100, embedding_dim=50, learning_rate=0.025, window_size=2)
    w2v.train(sentences, epochs=10)
    
    print("Word2Vec model trained!")
    
    # ç¤ºä¾‹ï¼šå‘é‡è¿ç®—
    # print("king - man + woman =", w2v.vector_math(['king'], ['man', 'woman']))
                            </code></pre>
                        </div>
                    </div>
                    <div id="w2v-go" class="code-tab-content">
                        <div class="code-block line-numbers">
                            <pre><code class="language-go">
package main

import (
    "fmt"
    "math"
    "math/rand"
)

type Word2Vec struct {
    vocabSize     int
    embeddingDim  int
    learningRate  float64
    windowSize    int
    W             [][]float64
    WPrime        [][]float64
}

func NewWord2Vec(vocabSize, embeddingDim int, learningRate float64, windowSize int) *Word2Vec {
    w2v := &Word2Vec{
        vocabSize:    vocabSize,
        embeddingDim: embeddingDim,
        learningRate: learningRate,
        windowSize:   windowSize,
    }
    
    // Xavieråˆå§‹åŒ–
    scale := math.Sqrt(2.0 / float64(embeddingDim))
    w2v.W = make([][]float64, vocabSize)
    w2v.WPrime = make([][]float64, vocabSize)
    
    for i := range w2v.W {
        w2v.W[i] = make([]float64, embeddingDim)
        w2v.WPrime[i] = make([]float64, embeddingDim)
        for j := range w2v.W[i] {
            w2v.W[i][j] = (rand.Float64()*2 - 1) * scale
            w2v.WPrime[i][j] = (rand.Float64()*2 - 1) * scale
        }
    }
    
    return w2v
}

func (w2v *Word2Vec) sigmoid(x float64) float64 {
    if x > 500 {
        return 1.0
    }
    if x < -500 {
        return 0.0
    }
    return 1 / (1 + math.Exp(-x))
}

func (w2v *Word2Vec) dot(v1, v2 []float64) float64 {
    var sum float64
    for i := range v1 {
        sum += v1[i] * v2[i]
    }
    return sum
}

func (w2v *Word2Vec) trainSample(centerWord, targetWord int, label float64) {
    h := w2v.W[centerWord]
    v := w2v.WPrime[targetWord]
    
    score := w2v.sigmoid(w2v.dot(h, v))
    error := label - score
    
    // æ›´æ–°å‘é‡
    for i := 0; i < w2v.embeddingDim; i++ {
        gradient := w2v.learningRate * error
        h[i] += gradient * v[i]
        v[i] += gradient * h[i]
    }
    
    w2v.W[centerWord] = h
    w2v.WPrime[targetWord] = v
}

func (w2v *Word2Vec) updateVectors(centerWord, contextWord int) {
    // æ­£æ ·æœ¬
    w2v.trainSample(centerWord, contextWord, 1)
    
    // è´Ÿæ ·æœ¬
    for k := 0; k < 5; k++ {
        negWord := rand.Intn(w2v.vocabSize)
        if negWord != centerWord && negWord != contextWord {
            w2v.trainSample(centerWord, negWord, -1)
        }
    }
}

func (w2v *Word2Vec) Train(sentences [][]int, epochs int) {
    for epoch := 0; epoch < epochs; epoch++ {
        for _, sentence := range sentences {
            for i, centerWord := range sentence {
                for j, contextWord := range sentence {
                    if i != j {
                        w2v.updateVectors(centerWord, contextWord)
                    }
                }
            }
        }
        fmt.Printf("Epoch %d completed\n", epoch+1)
    }
}

func (w2v *Word2Vec) CosineSimilarity(word1, word2 int) float64 {
    v1 := w2v.W[word1]
    v2 := w2v.W[word2]
    
    dot := w2v.dot(v1, v2)
    mag1 := math.Sqrt(w2v.dot(v1, v1))
    mag2 := math.Sqrt(w2v.dot(v2, v2))
    
    return dot / (mag1 * mag2)
}

func (w2v *Word2Vec) MostSimilar(word int, topN int) []struct {
    Word       int
    Similarity float64
} {
    wordVec := w2v.W[word]
    magWord := math.Sqrt(w2v.dot(wordVec, wordVec))
    
    similarities := make([]struct {
        Word       int
        Similarity float64
    }, 0, topN)
    
    for i := 0; i < w2v.vocabSize; i++ {
        if i != word {
            v := w2v.W[i]
            mag := math.Sqrt(w2v.dot(v, v))
            sim := w2v.dot(wordVec, v) / (magWord * mag)
            similarities = append(similarities, struct {
                Word       int
                Similarity float64
            }{i, sim})
        }
    }
    
    // æ’åº
    for i := 0; i < len(similarities)-1; i++ {
        for j := i + 1; j < len(similarities); j++ {
            if similarities[j].Similarity > similarities[i].Similarity {
                similarities[i], similarities[j] = similarities[j], similarities[i]
            }
        }
    }
    
    if len(similarities) > topN {
        return similarities[:topN]
    }
    return similarities
}

func main() {
    // ç¤ºä¾‹å¥å­ï¼ˆéœ€è¦å…ˆå»ºç«‹è¯ç´¢å¼•æ˜ å°„ï¼‰
    sentences := [][]int{
        {0, 1, 2, 3, 4, 5},
        {0, 6, 7, 8, 9, 10},
        {11, 12, 13, 14, 15},
        {16, 1, 17, 16, 18, 19},
    }
    
    w2v := NewWord2Vec(100, 50, 0.025, 2)
    w2v.Train(sentences, 10)
    
    fmt.Println("Word2Vec model trained!")
}
                            </code></pre>
                        </div>
                    </div>
                </div>
            </div>

            <h2>Word2Vec vs å…¶ä»–æ–¹æ³•å¯¹æ¯”</h2>
            <div class="algorithm-content">
                <table class="comparison-table">
                    <tr>
                        <th>ç‰¹æ€§</th>
                        <th>Word2Vec</th>
                        <th>GloVe</th>
                        <th>FastText</th>
                    </tr>
                    <tr>
                        <td>æå‡ºè€…</td>
                        <td>Google</td>
                        <td>Stanford</td>
                        <td>Facebook</td>
                    </tr>
                    <tr>
                        <td>è®­ç»ƒç›®æ ‡</td>
                        <td>é¢„æµ‹æ¦‚ç‡</td>
                        <td>å…±ç°æ¦‚ç‡æ¯”</td>
                        <td>å­è¯é¢„æµ‹</td>
                    </tr>
                    <tr>
                        <td>å¤„ç†ç”Ÿåƒ»è¯</td>
                        <td>âŒ å·®</td>
                        <td>âŒ å·®</td>
                        <td>âœ… å¥½</td>
                    </tr>
                    <tr>
                        <td>è®­ç»ƒé€Ÿåº¦</td>
                        <td>å¿«</td>
                        <td>ä¸­ç­‰</td>
                        <td>å¿«</td>
                    </tr>
                    <tr>
                        <td>å‘é‡è´¨é‡</td>
                        <td>å¥½</td>
                        <td>å¾ˆå¥½</td>
                        <td>å¾ˆå¥½</td>
                    </tr>
                </table>
            </div>

            <h2>å®é™…åº”ç”¨åœºæ™¯</h2>
            <div class="algorithm-content">
                <h3>1. è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—</h3>
                <p>è®¡ç®—è¯è¯­ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œç”¨äºæœç´¢ã€æ¨èç³»ç»Ÿã€‚</p>
                
                <h3>2. æ–‡æœ¬åˆ†ç±»</h3>
                <p>å°†æ–‡æœ¬è¡¨ç¤ºä¸ºè¯å‘é‡å‡å€¼ï¼Œä½œä¸ºåˆ†ç±»å™¨çš„è¾“å…¥ç‰¹å¾ã€‚</p>
                
                <h3>3. å‘½åå®ä½“è¯†åˆ«</h3>
                <p>åˆ©ç”¨è¯å‘é‡æ•æ‰è¯è¯­çš„è¯­ä¹‰ç‰¹å¾ï¼Œæå‡ NER å‡†ç¡®ç‡ã€‚</p>
                
                <h3>4. æœºå™¨ç¿»è¯‘</h3>
                <p>åœ¨å…±äº«çš„å‘é‡ç©ºé—´ä¸­å»ºç«‹ä¸åŒè¯­è¨€è¯è¯­çš„å¯¹åº”å…³ç³»ã€‚</p>
                
                <h3>5. æƒ…æ„Ÿåˆ†æ</h3>
                <p>é€šè¿‡è¯å‘é‡ç»„åˆæ•æ‰æ–‡æœ¬çš„æƒ…æ„Ÿææ€§ã€‚</p>
            </div>

            <h2>Word2Vec çš„å±€é™æ€§</h2>
            <div class="algorithm-content">
                <ul>
                    <li><strong>æ— æ³•å¤„ç†å¤šä¹‰è¯</strong>ï¼šæ¯ä¸ªè¯åªæœ‰å”¯ä¸€ä¸€ä¸ªå‘é‡</li>
                    <li><strong>æ— æ³•åˆ©ç”¨å…¨å±€ç»Ÿè®¡ä¿¡æ¯</strong>ï¼šåªå…³æ³¨å±€éƒ¨ä¸Šä¸‹æ–‡</li>
                    <li><strong>è¯åºä¿¡æ¯ä¸¢å¤±</strong>ï¼šCBOW å’Œ Skip-gram éƒ½ä¸è€ƒè™‘è¯åº</li>
                    <li><strong>é™æ€è¡¨ç¤º</strong>ï¼šæ— æ³•æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´è¯ä¹‰</li>
                </ul>
                <p style="color: #e74c3c; margin-top: 10px;">
                    ğŸ’¡ è¿™äº›å±€é™æ€§ä¿ƒè¿›äº† ELMoã€BERTã€GPT ç­‰åŠ¨æ€è¯å‘é‡æ¨¡å‹çš„å‘å±•
                </p>
            </div>

            <h2>æ€»ç»“ï¼šWord2Vec çš„æ ¸å¿ƒæ´å¯Ÿ</h2>
            <div class="concept-box">
                <h4>å…³é”®åˆ›æ–°</h4>
                <ol>
                    <li><strong>åˆ†å¸ƒå¼è¡¨ç¤º</strong>ï¼šå°†è¯è¡¨ç¤ºä¸ºç¨ å¯†å‘é‡ï¼Œè€Œéç¨€ç–ç‹¬çƒ­ç¼–ç </li>
                    <li><strong>ä¸Šä¸‹æ–‡å­¦ä¹ </strong>ï¼šé€šè¿‡é¢„æµ‹ä¸Šä¸‹æ–‡æ¥å­¦ä¹ è¯çš„è¯­ä¹‰è¡¨ç¤º</li>
                    <li><strong>é«˜æ•ˆè®­ç»ƒ</strong>ï¼šè´Ÿé‡‡æ ·å’Œå±‚æ¬¡ Softmax å¤§å¤§é™ä½äº†è®¡ç®—å¤æ‚åº¦</li>
                    <li><strong>è¯­ä¹‰è¿ç®—</strong>ï¼šè¯å‘é‡æ”¯æŒæœ‰æ„ä¹‰çš„ä»£æ•°è¿ç®—</li>
                    <li><strong>è¿ç§»å­¦ä¹ </strong>ï¼šé¢„è®­ç»ƒçš„è¯å‘é‡å¯ä»¥è¿ç§»åˆ°å„ç§ NLP ä»»åŠ¡</li>
                </ol>
            </div>

            <h3>å­¦ä¹ å»ºè®®</h3>
            <div class="algorithm-content">
                <ul>
                    <li>ç†è§£åˆ†å¸ƒå¼è¯­ä¹‰å‡è®¾ï¼šä¸€ä¸ªè¯çš„æ„ä¹‰ç”±å…¶ä¸Šä¸‹æ–‡å†³å®š</li>
                    <li>æŒæ¡ CBOW å’Œ Skip-gram ä¸¤ç§æ¶æ„çš„åŸç†å’ŒåŒºåˆ«</li>
                    <li>ç†è§£è´Ÿé‡‡æ ·å’Œå±‚æ¬¡ Softmax çš„ä¼˜åŒ–æ€æƒ³</li>
                    <li>ä½¿ç”¨ Gensim æˆ– TensorFlow å®ç° Word2Vec</li>
                    <li>åœ¨çœŸå®è¯­æ–™ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œè§‚å¯Ÿè¯å‘é‡çš„è¯­ä¹‰æ€§è´¨</li>
                    <li>å­¦ä¹ åç»­æ¨¡å‹ ELMoã€BERTï¼Œäº†è§£è¯å‘é‡æŠ€æœ¯çš„æ¼”è¿›</li>
                </ul>
            </div>
        </div>
    </section>

    <script src="../js/main.js"></script>
    <script src="../lib/prism/prism.min.js"></script>
    <script src="../lib/prism/prism-python.min.js"></script>
    <script src="../lib/prism/prism-go.min.js"></script>
    <script src="../lib/prism/prism-line-numbers.min.js"></script>
</body>
</html>
